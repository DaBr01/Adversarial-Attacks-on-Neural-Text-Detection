# Robustness of Generative AI Detection: Adversarial Attacks on Black-Box Neural Text Detectors

The increased quality and human-likeness of AI generated texts has resulted in a rising demand for neural text detectors, 
i.e. software that is able to detect whether a text was written by a human or generated by an AI. 
In our article "Robustness of Generative AI Detection: Adversarial Attacks on Black-Box Neural Text Detectors", we investigate a broad range of adversarial attacks in English texts with six different neural text detectors, including commercial and research tools. 
While the results show that no detector is completely invulnerable to adversarial attacks, the latest generation of commercial detectors proved to be very robust and not significantly influenced by most of the evaluated attack strategies. 

This repository contains the texts that have been generated for our evaluation of adversarial attacks on neural text detectors, as well as the code that was used to conduct the experiments.


## How to cite (to appear)
```
@article{fishchuk-braun-2024-robustness,
    title = "Robustness of Generative AI Detection: Adversarial Attacks on Black-Box Neural Text Detectors", 
    author = "Fishchuk, Vitalii and Braun, Daniel",
    journal = "International Journal of Speech Technology",
    year = "2024",
}
```
